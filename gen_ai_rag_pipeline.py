# -*- coding: utf-8 -*-
"""gen_ai_rag_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_0EeuL0f9THoqEzmitl8ROA2q6NQ9uBw
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Set path to your dataset
import pandas as pd

data_path = '/content/drive/MyDrive/genaifinal/final_data.csv'
df = pd.read_csv(data_path)

! pip install faiss-cpu langchain_community

# Import all necessary libraries
import pandas as pd
import requests
from PIL import Image, ImageDraw, ImageFont
from io import BytesIO
from tqdm import tqdm
import torch
import numpy as np
import faiss
import pickle
import random
import base64
import matplotlib.pyplot as plt
from IPython.display import display, HTML
import cv2

from transformers import CLIPProcessor, CLIPModel, AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_community.vectorstores import FAISS
from langchain_community.llms import HuggingFacePipeline
from langchain_community.docstore import InMemoryDocstore
from langchain.schema import Document
from langchain.prompts import PromptTemplate

# Preview and clean dataset
print(f"Dataset shape: {df.shape}")
print(df.head())
# Remove columns that are all NA values
df = df.dropna(axis=1, how='all')

# Drop unnecessary columns
columns_to_drop = ['Upc Ean Code', 'Is Amazon Seller', 'Variants', 'Model Number']
df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])

print(f"Cleaned dataset shape: {df.shape}")

# Initialize models
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# CLIP for multimodal embeddings
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

def combine_fields(row):
    """Combine multiple product fields into a single text description"""
    fields = [
        row.get("Product Name", ""),
        row.get("Brand", ""),
        row.get("Category", ""),
        str(row.get("Selling Price", "")),
        row.get("About Product", ""),
        row.get("Product Specification", ""),
        row.get("Technical Details", ""),
        row.get("Shipping Weight", ""),
        row.get("Features", ""),
    ]
    return " | ".join(str(f).strip() for f in fields if pd.notna(f) and str(f).strip())

def download_image(url):
    """Download and return PIL Image from URL"""
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        img = Image.open(BytesIO(response.content)).convert("RGB")
        return img
    except Exception as e:
        print(f"Failed to fetch image from {url}: {e}")
        return None

def get_clip_embedding(text, image):
    """Generate CLIP embedding from text and image"""
    try:
        # Process text
        text_inputs = clip_processor(text=[text], return_tensors="pt", truncation=True, padding=True).to(device)
        with torch.no_grad():
            text_features = clip_model.get_text_features(**text_inputs)

        # Process image
        image_inputs = clip_processor(images=image, return_tensors="pt").to(device)
        with torch.no_grad():
            image_features = clip_model.get_image_features(**image_inputs)

        # Combine and normalize
        combined = text_features + image_features
        combined = combined / combined.norm(p=2, dim=-1, keepdim=True)
        return combined[0].cpu().numpy()
    except Exception as e:
        print(f"Error generating embedding: {e}")
        return None

def analyze_image_features(image):
    """Extract visual features and characteristics from an image"""
    try:
        # Convert PIL to OpenCV format for additional analysis
        img_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)

        # Basic image properties
        height, width = img_cv.shape[:2]

        # Color analysis
        avg_color = np.mean(img_cv, axis=(0, 1))
        dominant_color = "Blue" if avg_color[0] > avg_color[1] and avg_color[0] > avg_color[2] else \
                        "Green" if avg_color[1] > avg_color[2] else "Red"

        # Brightness analysis
        gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)
        brightness = np.mean(gray)
        brightness_level = "Bright" if brightness > 127 else "Dark"

        features = {
            "dimensions": f"{width}x{height}",
            "dominant_color": dominant_color,
            "brightness": brightness_level,
            "aspect_ratio": round(width/height, 2)
        }

        return features
    except Exception as e:
        print(f"Error analyzing image: {e}")
        return {}

# Create embeddings and documents
docs = []
embeddings = []
product_categories = []
product_images = {}  # Store image URLs for later retrieval

# Process dataset in batches
batch_size = 16
for i in tqdm(range(0, len(df), batch_size)):
    batch = df.iloc[i:i+batch_size]

    for idx, row in batch.iterrows():
        text = combine_fields(row)
        if not text:
            continue

        image_url = row.get("Image", "")
        image = download_image(image_url)
        if image is None:
            continue

        embedding = get_clip_embedding(text, image)
        if embedding is None:
            continue

        visual_features = analyze_image_features(image)

        # Create document without the image caption
        doc = Document(
            page_content=text,
            metadata={
                "source": image_url,
                "category": row.get("Category", "Unknown"),
                "price": row.get("Selling Price", ""),
                "brand": row.get("Brand", ""),
                # "image_caption": caption,  # â† Removed
                "visual_features": visual_features,
                "product_id": str(idx)
            }
        )
        docs.append(doc)
        embeddings.append(embedding)
        product_categories.append(row.get("Category", "Unknown"))
        product_images[str(idx)] = image_url

print(f"Created {len(docs)} embeddings/documents.")

# Save all data
save_path = "/content/drive/MyDrive/genaifinal/"
with open(f"{save_path}embeddings.pkl", "wb") as f:
    pickle.dump(embeddings, f)
with open(f"{save_path}docs.pkl", "wb") as f:
    pickle.dump(docs, f)
with open(f"{save_path}categories.pkl", "wb") as f:
    pickle.dump(product_categories, f)
with open(f"{save_path}product_images.pkl", "wb") as f:
    pickle.dump(product_images, f)

save_path = "/content/drive/MyDrive/genaifinal/"
with open(f"{save_path}embeddings.pkl", "rb") as f:
    embeddings = pickle.load(f)
with open(f"{save_path}docs.pkl", "rb") as f:
    docs = pickle.load(f)
with open(f"{save_path}categories.pkl", "rb") as f:
    categories = pickle.load(f)
with open(f"{save_path}product_images.pkl", "rb") as f:
    product_images = pickle.load(f)

# Build FAISS index
if embeddings:
    dimension = embeddings[0].shape[0]
    index = faiss.IndexFlatIP(dimension)
    index.add(np.stack(embeddings))

    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})

    vectorstore = FAISS(
        index=index,
        docstore=docstore,
        index_to_docstore_id={i: str(i) for i in range(len(docs))},
        embedding_function=None
    )

    vectorstore.save_local("multimodal_faiss_index")
    print("FAISS vectorstore saved.")

def tokenize_category(cat_str):
    """Split category string into a set of subcategories."""
    if not isinstance(cat_str, str):
        return set()  # or return set(['unknown']) if you want a placeholder
    return set([part.strip() for part in cat_str.split('|')])


def categories_match(true_cat, pred_cat):
    """Return True if true and predicted categories share at least one subcategory."""
    true_set = tokenize_category(true_cat)
    pred_set = tokenize_category(pred_cat)
    return len(true_set.intersection(pred_set)) > 0

def evaluate_retrieval_flexible(index, docs, labels, embeddings, k_values=[1, 5, 10]):
    recalls = {k: 0 for k in k_values}
    top1_correct = 0
    total = len(docs)

    y_true = []
    y_pred = []

    for i, (query_vec, true_label) in enumerate(zip(embeddings, labels)):
        query_vec = np.expand_dims(query_vec, axis=0)
        D, I = index.search(query_vec, max(k_values) + 1)  # get top k+1 to avoid self if present
        I = I[0]

        # Remove self index if present
        retrieved_indices = [j for j in I if j != i]
        retrieved_labels = [labels[j] for j in retrieved_indices]

        # Top-1 prediction flexible matching
        if retrieved_labels:
            predicted_label = retrieved_labels[0]
            y_true.append(true_label)
            y_pred.append(predicted_label)
            if categories_match(true_label, predicted_label):
                top1_correct += 1

        # Recall@k flexible matching
        for k in k_values:
            topk_labels = retrieved_labels[:k]
            if any(categories_match(true_label, pred) for pred in topk_labels):
                recalls[k] += 1

    accuracy = top1_correct / total
    recall_scores = {k: recalls[k] / total for k in k_values}

    print(f"\nTop-1 Accuracy (flexible): {accuracy:.3f}")
    print("Recall@k (flexible):", recall_scores)

    return accuracy, recall_scores

accuracy, recall_scores = evaluate_retrieval_flexible(index, docs, categories, embeddings)

from huggingface_hub import login
login("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")

# Initialize LLM
try:
    llama_tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3.1-8B-Instruct")
    if llama_tokenizer.pad_token is None:
        llama_tokenizer.pad_token = llama_tokenizer.eos_token

    llama_model = AutoModelForCausalLM.from_pretrained(
        "meta-llama/Meta-Llama-3.1-8B-Instruct",
        device_map="auto",
        torch_dtype=torch.float16
    )
    llm_pipeline = pipeline(
        "text-generation",
        model=llama_model,
        tokenizer=llama_tokenizer
    )
    print("LLM loaded successfully.")
except Exception as e:
    print(f"Error loading LLM: {e}")
    llm_pipeline = None

save_dir = "./meta_llama_saved"

llama_tokenizer.save_pretrained(save_dir)
llama_model.save_pretrained(save_dir)

print(f"Model and tokenizer saved to {save_dir}")

!zip -r meta_llama_saved.zip meta_llama_saved

from langchain.prompts import PromptTemplate

prompt_templates = {
    "image_identification": PromptTemplate.from_template("""
You are a knowledgeable product identification assistant. A user has uploaded an image, and your task is to help them identify the product by analyzing its visual features and comparing it with similar products.

**Visual Features Extracted:**
{visual_features}

**Similar Products in Database:**
{context}

**User Question:**
{question}

Please respond with:
1. Likely product name or category
2. Distinct visual traits you observed
3. Similar or alternative product suggestions
4. Estimated price range based on matches
5. Where the user can find or purchase similar items

Answer:
"""),

    "visual_search": PromptTemplate.from_template("""
You are a smart visual shopping assistant. A user uploaded an image and is seeking product recommendations. Analyze the visual input and match it to similar products in the catalog.

**Extracted Visual Cues:**
{visual_features}

**Similar Items Found:**
{context}

**User Request:**
{question}

Your response should include:
1. Top 3 most visually similar products, with brief justifications
2. Matching visual or design elements
3. Price comparisons between these options
4. Alternative products if the exact match isn't available

Suggested Products:
"""),

    "product_comparison": PromptTemplate.from_template("""
You are a product expert comparing visually and textually similar items. The user needs help deciding between options.

**Visual and Text Features:**
{visual_features}

**Products to Compare:**
{context}

**User's Question:**
{question}

Compare the products by providing:
1. A side-by-side comparison of key features
2. Notable visual similarities and differences
3. Pros and cons of each item
4. Ideal use cases for each option
5. Assessment of value for money

Comparison Summary:
"""),

    "product_inquiry": PromptTemplate.from_template("""
You are a trusted e-commerce assistant. Use the information below to help the user understand more about a product or make a better purchase decision.

**Product Details Available:**
{context}

**Customer's Question:**
{question}

Respond with:
1. A direct and helpful answer
2. Relevant features and specifications
3. Any visual characteristics that support the answer
4. Related suggestions or alternatives if appropriate

Answer:
""")
}

class EnhancedMultimodalRAGChatbot:
    def __init__(self, vectorstore, llm_pipeline, clip_model, clip_processor):
        self.vectorstore = vectorstore
        self.llm_pipeline = llm_pipeline
        self.clip_model = clip_model
        self.clip_processor = clip_processor
        self.device = device

    def generate_query_embedding(self, query_text=None, query_image=None):
        """Generate embedding for text or image query"""
        try:
            if query_text and query_image:
                # Combined text and image query
                text_inputs = self.clip_processor(text=[query_text], return_tensors="pt", truncation=True).to(self.device)
                with torch.no_grad():
                    text_features = self.clip_model.get_text_features(**text_inputs)

                image_inputs = self.clip_processor(images=query_image, return_tensors="pt").to(self.device)
                with torch.no_grad():
                    image_features = self.clip_model.get_image_features(**image_inputs)

                combined = text_features + image_features
                embedding = combined / combined.norm(p=2, dim=-1, keepdim=True)
                return embedding[0].cpu().numpy()

            elif query_text:
                text_inputs = self.clip_processor(text=[query_text], return_tensors="pt", truncation=True).to(self.device)
                with torch.no_grad():
                    text_features = self.clip_model.get_text_features(**text_inputs)
                embedding = text_features / text_features.norm(p=2, dim=-1, keepdim=True)
                return embedding[0].cpu().numpy()

            elif query_image:
                image_inputs = self.clip_processor(images=query_image, return_tensors="pt").to(self.device)
                with torch.no_grad():
                    image_features = self.clip_model.get_image_features(**image_inputs)
                embedding = image_features / image_features.norm(p=2, dim=-1, keepdim=True)
                return embedding[0].cpu().numpy()

        except Exception as e:
            print(f"Error generating query embedding: {e}")
            return None

    def identify_product_from_image(self, image, additional_query=""):
        """Identify a product from an uploaded image"""
        # Generate image caption and features
        visual_features = analyze_image_features(image)

        # Generate embedding for the image
        query_embedding = self.generate_query_embedding(query_image=image)
        if query_embedding is None:
            return "Error processing the image. Please try again."

        # Retrieve similar products
        try:
            results = self.vectorstore.similarity_search_by_vector(query_embedding, k=5)
            context = "\n\n".join([
                f"Product {i+1}:\n{r.page_content}\n"
                f"Image Caption: {r.metadata.get('image_caption', 'N/A')}\n"
                f"Visual Features: {r.metadata.get('visual_features', {})}"
                for i, r in enumerate(results)
            ])

            # Format prompt for image identification
            prompt_template = prompt_templates["image_identification"]
            question = additional_query if additional_query else "What product is this? Can you identify it and provide details?"

            formatted_prompt = prompt_template.format(
                context=context,
                question=question,
                visual_features=visual_features
            )

            # Generate response
            if self.llm_pipeline:
                response = self.llm_pipeline(
                    formatted_prompt,
                    max_new_tokens=400,
                    do_sample=True,
                    temperature=0.7,
                    pad_token_id=self.llm_pipeline.tokenizer.eos_token_id
                )[0]['generated_text']

                response = response[len(formatted_prompt):].strip()
            else:
                response = f"Image Analysis:\nFeatures: {visual_features}\n\nSimilar Products:\n{context}"

            return {
                "response": response,
                "visual_features": visual_features,
                "similar_products": results,
                "confidence": "High" if len(results) > 2 else "Medium"
            }

        except Exception as e:
            print(f"Error in product identification: {e}")
            return "Sorry, I couldn't identify the product. Please try with a clearer image."

    def visual_search(self, query_image, query_text=""):
        """Search for products similar to the uploaded image"""
        query_embedding = self.generate_query_embedding(query_text, query_image)
        if query_embedding is None:
            return "Error processing your search. Please try again."

        try:
            results = self.vectorstore.similarity_search_by_vector(query_embedding, k=6)

            # Generate image analysis
            visual_features = analyze_image_features(query_image)

            context = "\n\n".join([
                f"Product {i+1}:\n{r.page_content}\n"
                f"Price: {r.metadata.get('price', 'N/A')}\n"
                f"Brand: {r.metadata.get('brand', 'N/A')}"
                for i, r in enumerate(results)
            ])

            prompt_template = prompt_templates["visual_search"]
            question = query_text if query_text else "Find products similar to this image"

            formatted_prompt = prompt_template.format(
                context=context,
                question=question,
                visual_features=visual_features
            )

            if self.llm_pipeline:
                response = self.llm_pipeline(
                    formatted_prompt,
                    max_new_tokens=400,
                    do_sample=True,
                    temperature=0.7,
                    pad_token_id=self.llm_pipeline.tokenizer.eos_token_id
                )[0]['generated_text']

                response = response[len(formatted_prompt):].strip()
            else:
                response = f"Visual Search Results:\n{context}"

            return {
                "response": response,
                "search_results": results,
                "query_analysis": {"features": visual_features}
            }

        except Exception as e:
            print(f"Error in visual search: {e}")
            return "Sorry, visual search encountered an error."

    def chat(self, query_text=None, query_image=None, query_type="general", k=3):
        """Enhanced chat function with computer vision capabilities"""
        if not query_text and not query_image:
            return "Please provide either a text query, an image, or both."

        # Handle different query types
        if query_image and query_type == "identify":
            return self.identify_product_from_image(query_image, query_text or "")

        elif query_image and query_type == "search":
            return self.visual_search(query_image, query_text or "")

        elif query_image and query_type == "general":
            # Determine intent based on query text
            if query_text:
                if any(word in query_text.lower() for word in ["what is", "identify", "recognize", "what product"]):
                    return self.identify_product_from_image(query_image, query_text)
                elif any(word in query_text.lower() for word in ["find", "search", "similar", "like this"]):
                    return self.visual_search(query_image, query_text)
            else:
                # Default to identification for image-only queries
                return self.identify_product_from_image(query_image, "")

        # Handle text-only queries (original functionality)
        query_embedding = self.generate_query_embedding(query_text, query_image)
        if query_embedding is None:
            return "Error processing your query. Please try again."

        try:
            results = self.vectorstore.similarity_search_by_vector(query_embedding, k=k)
            context = "\n\n".join([f"Product {i+1}:\n{r.page_content}" for i, r in enumerate(results)])

            prompt_template = prompt_templates["product_inquiry"]
            formatted_prompt = prompt_template.format(context=context, question=query_text)

            if self.llm_pipeline:
                response = self.llm_pipeline(
                    formatted_prompt,
                    max_new_tokens=350,
                    do_sample=True,
                    temperature=0.7,
                    pad_token_id=self.llm_pipeline.tokenizer.eos_token_id
                )[0]['generated_text']

                response = response[len(formatted_prompt):].strip()
            else:
                response = f"Retrieved products:\n{context}"

            return {
                "response": response,
                "retrieved_products": results,
                "num_results": len(results)
            }

        except Exception as e:
            print(f"Error in chat function: {e}")
            return "Sorry, I encountered an error processing your request."

    def display_results_with_images(self, results):
        """Display search results with images"""
        for i, result in enumerate(results.get('similar_products', results.get('retrieved_products', []))):
            print(f"\n--- Product {i+1} ---")
            print(f"Description: {result.page_content[:200]}...")
            print(f"Price: {result.metadata.get('price', 'N/A')}")
            print(f"Brand: {result.metadata.get('brand', 'N/A')}")

            # Try to display image
            if 'source' in result.metadata:
                try:
                    img = download_image(result.metadata['source'])
                    if img:
                        img.thumbnail((200, 200))
                        display(img)
                except:
                    print("Could not display image")